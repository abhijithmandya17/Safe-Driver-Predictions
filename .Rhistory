df_b3 <- rbind(df_b3, c(25, 45))
plot(df_b3)
test3.lm <- lm(y~x, data = df_b3)
summary(influence.measures(test3.lm))
df_b3 =data.frame(cbind(x,y))
df_b3 <- rbind(df_b3, c(20, 45))
plot(df_b3)
test3.lm <- lm(y~x, data = df_b3)
summary(influence.measures(test3.lm))
df_b3 =data.frame(cbind(x,y))
df_b3 <- rbind(df_b3, c(30, 10))
plot(df_b3)
test3.lm <- lm(y~x, data = df_b3)
summary(influence.measures(test3.lm))
df_b3 =data.frame(cbind(x,y))
df_b3 <- rbind(df_b3, c(30, 15))
plot(df_b3)
test3.lm <- lm(y~x, data = df_b3)
summary(influence.measures(test3.lm))
df_b3 =data.frame(cbind(x,y))
df_b3 <- rbind(df_b3, c(10, 45))
plot(df_b3)
test3.lm <- lm(y~x, data = df_b3)
summary(influence.measures(test3.lm))
df_b3 =data.frame(cbind(x,y))
df_b3 <- rbind(df_b3, c(70, 45))
plot(df_b3)
test3.lm <- lm(y~x, data = df_b3)
summary(influence.measures(test3.lm))
df_b3 =data.frame(cbind(x,y))
df_b3 <- rbind(df_b3, c(65, 45))
plot(df_b3)
test3.lm <- lm(y~x, data = df_b3)
summary(influence.measures(test3.lm))
df_b3 =data.frame(cbind(x,y))
df_b3 <- rbind(df_b3, c(60, 45))
plot(df_b3)
test3.lm <- lm(y~x, data = df_b3)
summary(influence.measures(test3.lm))
df_b3 =data.frame(cbind(x,y))
df_b3 <- rbind(df_b3, c(30, 60))
plot(df_b3)
test3.lm <- lm(y~x, data = df_b3)
summary(influence.measures(test3.lm))
df_b3 =data.frame(cbind(x,y))
df_b3 <- rbind(df_b3, c(60, 60))
plot(df_b3)
test3.lm <- lm(y~x, data = df_b3)
summary(influence.measures(test3.lm))
df_b3 =data.frame(cbind(x,y))
df_b3 <- rbind(df_b3, c(60, 45))
plot(df_b3)
test3.lm <- lm(y~x, data = df_b3)
summary(influence.measures(test3.lm))
df_b3 =data.frame(cbind(x,y))
df_b3 <- rbind(df_b3, c(60, 1))
plot(df_b3)
test3.lm <- lm(y~x, data = df_b3)
summary(influence.measures(test3.lm))
df_b3 =data.frame(cbind(x,y))
df_b3 <- rbind(df_b3, c(60, 35))
plot(df_b3)
test3.lm <- lm(y~x, data = df_b3)
summary(influence.measures(test3.lm))
df_b3 =data.frame(cbind(x,y))
df_b3 <- rbind(df_b3, c(60, 40))
plot(df_b3)
test3.lm <- lm(y~x, data = df_b3)
summary(influence.measures(test3.lm))
df_b3 =data.frame(cbind(x,y))
df_b3 <- rbind(df_b3, c(60, 45))
plot(df_b3)
test3.lm <- lm(y~x, data = df_b3)
summary(influence.measures(test3.lm))
df_b3 =data.frame(cbind(x,y))
df_b3 <- rbind(df_b3, c(35, 200))
plot(df_b3)
test3.lm <- lm(y~x, data = df_b3)
summary(influence.measures(test3.lm))
df_b3 =data.frame(cbind(x,y))
df_b3 <- rbind(df_b3, c(-35, 45))
plot(df_b3)
test3.lm <- lm(y~x, data = df_b3)
summary(influence.measures(test3.lm))
df_b3 =data.frame(cbind(x,y))
df_b3 <- rbind(df_b3, c(-35, -45))
plot(df_b3)
test3.lm <- lm(y~x, data = df_b3)
summary(influence.measures(test3.lm))
df_b3 =data.frame(cbind(x,y))
df_b3 <- rbind(df_b3, c(35, -45))
plot(df_b3)
test3.lm <- lm(y~x, data = df_b3)
summary(influence.measures(test3.lm))
df_b3 =data.frame(cbind(x,y))
df_b3 <- rbind(df_b3, c(30, -45))
plot(df_b3)
test3.lm <- lm(y~x, data = df_b3)
summary(influence.measures(test3.lm))
df_b3 =data.frame(cbind(x,y))
df_b3 <- rbind(df_b3, c(37, 45))
plot(df_b3)
test3.lm <- lm(y~x, data = df_b3)
summary(influence.measures(test3.lm))
df_b3 =data.frame(cbind(x,y))
df_b3 <- rbind(df_b3, c(54, 45))
plot(df_b3)
test3.lm <- lm(y~x, data = df_b3)
summary(influence.measures(test3.lm))
par(xpd = NA)
par
plot(NA, NA, type = "n", xlim = c(0, 100), ylim = c(0, 100), xlab = "X", ylab = "Y")
lines(x = c(40, 40), y = c(0, 100))
text(x = 40, y = 108, labels = c("t1"), col = "red")
lines(x = c(0, 40), y = c(75, 75))
text(x = -8, y = 75, labels = c("t2"), col = "red")
lines(x = c(75, 75), y = c(0, 100))
text(x = 75, y = 108, labels = c("t3"), col = "red")
lines(x = c(20, 20), y = c(0, 75))
lines(x = c(20, 20), y = c(0, 75))
text(x = 20, y = 80, labels = c("t4"), col = "red")
lines(x = c(75, 100), y = c(25, 25))
text(x = 70, y = 25, labels = c("t5"), col = "red")
text(x = (40 + 75)/2, y = 50, labels = c("R1"))
text(x = 20, y = (100 + 75)/2, labels = c("R2"))
text(x = (75 + 100)/2, y = (100 + 25)/2, labels = c("R3"))
text(x = (75 + 100)/2, y = 25/2, labels = c("R4"))
text(x = 30, y = 75/2, labels = c("R5"))
text(x = 10, y = 75/2, labels = c("R6"))
p = seq(0, 1, 0.01)
gini = p * (1 - p) * 2
entropy = -(p * log(p) + (1 - p) * log(1 - p))
class.err = 1 - pmax(p, 1 - p)
matplot(p, cbind(gini, entropy, class.err), col = c("red", "green", "blue"))
gini
par(xpd = NA)
plot(NA, NA, type = "n", xlim = c(-2, 2), ylim = c(-3, 3), xlab = "X1", ylab = "X2")
# X2 < 1
lines(x = c(-2, 2), y = c(1, 1))
# X1 < 1 with X2 < 1
lines(x = c(1, 1), y = c(-3, 1))
text(x = (-2 + 1)/2, y = -1, labels = c(-1.8))
text(x = 1.5, y = -1, labels = c(0.63))
# X2 < 2 with X2 >= 1
lines(x = c(-2, 2), y = c(2, 2))
text(x = 0, y = 2.5, labels = c(2.49))
# X1 < 0 with X2<2 and X2>=1
lines(x = c(0, 0), y = c(1, 2))
text(x = -1, y = 1.5, labels = c(-1.06))
text(x = 1, y = 1.5, labels = c(0.21))
par(xpd = NA)
plot(NA, NA, type = "n", xlim = c(-2, 2), ylim = c(-3, 3), xlab = "X1", ylab = "X2")
lines(x = c(-2, 2), y = c(1, 1))
lines(x = c(1, 1), y = c(-3, 1))
text(x = (-2 + 1)/2, y = -1, labels = c(-1.8))
text(x = 1.5, y = -1, labels = c(0.63))
lines(x = c(-2, 2), y = c(2, 2))
text(x = 0, y = 2.5, labels = c(2.49))
# X1 < 0 with X2<2 and X2>=1
lines(x = c(0, 0), y = c(1, 2))
text(x = -1, y = 1.5, labels = c(-1.06))
text(x = 1, y = 1.5, labels = c(0.21))
p = c(0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75)
# X2 < 1
sum(p >= 0.5) > sum(p < 0.5)
mean(p) #
library(MASS)
library(randomForest)
train = sample(dim(Boston)[1], dim(Boston)[1]/2)
p = dim(Boston)[2] - 1
train = sample(dim(Boston)[1], dim(Boston)[1]/2)
X.train = Boston[train, -14]
X.test = Boston[-train, -14]
Y.train = Boston[train, 14]
Y.test = Boston[-train, 14]
p = dim(Boston)[2] - 1
p
p.2 = p/2
p.2
p.sq
p.sq = sqrt(p)
p.sq
rf.boston.p = randomForest(X.train, Y.train, xtest = X.test, ytest = Y.test,
mtry = p, ntree = 500)
rf.boston.p.2 = randomForest(X.train, Y.train, xtest = X.test, ytest = Y.test,
mtry = p.2, ntree = 500)
rf.boston.p.sq = randomForest(X.train, Y.train, xtest = X.test, ytest = Y.test,
mtry = p.sq, ntree = 500)
plot(1:500, rf.boston.p$test$mse, col = "green", type = "l", xlab = "Number of Trees",
ylab = "Test MSE", ylim = c(10, 19))
lines(1:500, rf.boston.p.2$test$mse, col = "red", type = "l")
lines(1:500, rf.boston.p.sq$test$mse, col = "blue", type = "l")
legend("topright", c("m=p", "m=p/2", "m=sqrt(p)"), col = c("green", "red", "blue"),
cex = 1, lty = 1)
plot(1:500, rf.boston.p$test$mse, col = "green", type = "l", xlab = "Number of Trees",
ylab = "Test MSE", ylim = c(10, 25))
lines(1:500, rf.boston.p.2$test$mse, col = "red", type = "l")
lines(1:500, rf.boston.p.sq$test$mse, col = "blue", type = "l")
legend("topright", c("m=p", "m=p/2", "m=sqrt(p)"), col = c("green", "red", "blue"),
cex = 1, lty = 1)
plot(1:500, rf.boston.p$test$mse, col = "green", type = "l", xlab = "Number of Trees",
ylab = "Test MSE", ylim = c(10, 500))
lines(1:500, rf.boston.p.2$test$mse, col = "red", type = "l")
lines(1:500, rf.boston.p.sq$test$mse, col = "blue", type = "l")
legend("topright", c("m=p", "m=p/2", "m=sqrt(p)"), col = c("green", "red", "blue"),
cex = 1, lty = 1)
plot(1:500, rf.boston.p$test$mse, col = "green", type = "l", xlab = "Number of Trees",
ylab = "Test MSE", ylim = c(10, 50))
lines(1:500, rf.boston.p.2$test$mse, col = "red", type = "l")
lines(1:500, rf.boston.p.sq$test$mse, col = "blue", type = "l")
legend("topright", c("m=p", "m=p/2", "m=sqrt(p)"), col = c("green", "red", "blue"),
cex = 1, lty = 1)
plot(1:500, rf.boston.p$test$mse, col = "green", type = "l", xlab = "Number of Trees",
ylab = "Test MSE", ylim = c(10, 40))
lines(1:500, rf.boston.p.2$test$mse, col = "red", type = "l")
lines(1:500, rf.boston.p.sq$test$mse, col = "blue", type = "l")
legend("topright", c("m=p", "m=p/2", "m=sqrt(p)"), col = c("green", "red", "blue"),
cex = 1, lty = 1)
library(ISLR)
attach(Carseats)
set.seed(1)
train = sample(dim(Carseats)[1], dim(Carseats)[1]/2)
Carseats.train = Carseats[train, ]
Carseats.test = Carseats[-train, ]
library(tree)
tree.carseats = tree(Sales ~ ., data = Carseats.train)
summary(tree.carseats)
tree.carseats = tree(Sales ~ ., data = Carseats.train)
library(tree)
install.packages(tree)
install.packages("tree")
library(tree)
tree.carseats = tree(Sales ~ ., data = Carseats.train)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
pred.carseats = predict(tree.carseats, Carseats.test)
mean((Carseats.test$Sales - pred.carseats)^2)
cv.carseats = cv.tree(tree.carseats, FUN = prune.tree)
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")
pruned.carseats = prune.tree(tree.carseats, best = 9)
par(mfrow = c(1, 1))
plot(pruned.carseats)
text(pruned.carseats, pretty = 0)
pred.pruned = predict(pruned.carseats, Carseats.test)
mean((Carseats.test$Sales - pred.pruned)^2)
bag.carseats = randomForest(Sales ~ ., data = Carseats.train, mtry = 10, ntree = 500,
importance = T)
bag.pred = predict(bag.carseats, Carseats.test)
mean((Carseats.test$Sales - bag.pred)^2)
importance(bag.carseats)
rf.carseats = randomForest(Sales ~ ., data = Carseats.train, mtry = 5, ntree = 500,
importance = T)
rf.pred = predict(rf.carseats, Carseats.test)
mean((Carseats.test$Sales - rf.pred)^2)
importance(rf.carseats)
attach(OJ)
set.seed(1013)
train = sample(dim(OJ)[1], 800)
OJ.train = OJ[train, ]
OJ.test = OJ[-train, ]
oj.tree = tree(Purchase ~ ., data = OJ.train)
summary(oj.tree)
oj.tree
plot(oj.tree)
text(oj.tree, pretty = 0)
oj.pred = predict(oj.tree, OJ.test, type = "class")
table(OJ.test$Purchase, oj.pred)
cv.oj = cv.tree(oj.tree, FUN = prune.tree)
cv.oj
plot(cv.oj$size, cv.oj$dev, type = "b", xlab = "Tree Size", ylab = "Deviance")
oj.pruned = prune.tree(oj.tree, best = 6)
summary(oj.pruned)
pred.unpruned = predict(oj.tree, OJ.test, type = "class")
misclass.unpruned = sum(OJ.test$Purchase != pred.unpruned)
misclass.unpruned/length(pred.unpruned)
pred.pruned = predict(oj.pruned, OJ.test, type = "class")
misclass.pruned = sum(OJ.test$Purchase != pred.pruned)
misclass.pruned/length(pred.pruned)
sum(is.na(Hitters$Salary))
Hitters = Hitters[-which(is.na(Hitters$Salary)), ]
sum(is.na(Hitters$Salary))
Hitters$Salary = log(Hitters$Salary)
Hitters$Salary
train = 1:200
Hitters.train = Hitters[train, ]
Hitters.test = Hitters[-train, ]
library(gbm)
set.seed(103)
pows = seq(-10, -0.2, by = 0.1)
lambdas = 10^pows
length.lambdas = length(lambdas)
train.errors = rep(NA, length.lambdas)
test.errors = rep(NA, length.lambdas)
for (i in 1:length.lambdas) {
boost.hitters = gbm(Salary ~ ., data = Hitters.train, distribution = "gaussian",
n.trees = 1000, shrinkage = lambdas[i])
train.pred = predict(boost.hitters, Hitters.train, n.trees = 1000)
test.pred = predict(boost.hitters, Hitters.test, n.trees = 1000)
train.errors[i] = mean((Hitters.train$Salary - train.pred)^2)
test.errors[i] = mean((Hitters.test$Salary - test.pred)^2)
}
plot(lambdas, train.errors, type = "b", xlab = "Shrinkage", ylab = "Train MSE",
col = "blue", pch = 20)
plot(lambdas, test.errors, type = "b", xlab = "Shrinkage", ylab = "Test MSE",
col = "red", pch = 20)
min(test.errors)
# 0.2561
lambdas[which.min(test.errors)]
# 0.05012
lm.fit = lm(Salary ~ ., data = Hitters.train)
lm.pred = predict(lm.fit, Hitters.test)
mean((Hitters.test$Salary - lm.pred)^2)
library(glmnet)
set.seed(121)
x = model.matrix(Salary ~ ., data = Hitters.train)
y = Hitters.train$Salary
x.test = model.matrix(Salary ~ ., data = Hitters.test)
lasso.fit = glmnet(x, y, alpha = 1)
lasso.pred = predict(lasso.fit, s = 0.01, newx = x.test)
mean((Hitters.test$Salary - lasso.pred)^2)
boost.best = gbm(Salary ~ ., data = Hitters.train, distribution = "gaussian",
n.trees = 1000, shrinkage = lambdas[which.min(test.errors)])
summary(boost.best)
set.seed(21)
rf.hitters = randomForest(Salary ~ ., data = Hitters.train, ntree = 500, mtry = 19)
rf.pred = predict(rf.hitters, Hitters.test)
mean((Hitters.test$Salary - rf.pred)^2)
set.seed(1)
rf.hitters = randomForest(Salary ~ ., data = Hitters.train, ntree = 500, mtry = 19)
rf.pred = predict(rf.hitters, Hitters.test)
mean((Hitters.test$Salary - rf.pred)^2)
summary(Weekly)
train = sample(nrow(Weekly), 2/3 * nrow(Weekly))
test = -train
test
glm.fit = glm(Direction ~ . - Year - Today, data = Weekly[train, ], family = "binomial")
glm.probs = predict(glm.fit, newdata = Weekly[test, ], type = "response")
glm.pred = rep("Down", length(glm.probs))
glm.pred[glm.probs > 0.5] = "Up"
table(glm.pred, Weekly$Direction[test])
mean(glm.pred != Weekly$Direction[test])
Weekly$BinomialDirection = ifelse(Weekly$Direction == "Up", 1, 0)
boost.weekly = gbm(BinomialDirection ~ . - Year - Today - Direction, data = Weekly[train,
], distribution = "bernoulli", n.trees = 5000)
yhat.boost = predict(boost.weekly, newdata = Weekly[test, ], n.trees = 5000)
yhat.pred = rep(0, length(yhat.boost))
yhat.pred[yhat.boost > 0.5] = 1
table(yhat.pred, Weekly$BinomialDirection[test])
mean(yhat.pred != Weekly$BinomialDirection[test])
mean(glm.pred != Weekly$Direction[test])
Weekly = Weekly[, !(names(Weekly) %in% c("BinomialDirection"))]
bag.weekly = randomForest(Direction ~ . - Year - Today, data = Weekly, subset = train,
mtry = 6)
yhat.bag = predict(bag.weekly, newdata = Weekly[test, ])
table(yhat.bag, Weekly$Direction[test])
mean(yhat.bag != Weekly$Direction[test])
rf.weekly = randomForest(Direction ~ . - Year - Today, data = Weekly, subset = train,
mtry = 2)
yhat.bag = predict(rf.weekly, newdata = Weekly[test, ])
table(yhat.bag, Weekly$Direction[test])
mean(yhat.bag != Weekly$Direction[test])
# Chapter 8 Lab: Decision Trees
# Fitting Classification Trees
library(tree)
library(ISLR)
attach(Carseats)
High=ifelse(Sales<=8,"No","Yes")
Carseats=data.frame(Carseats,High)
tree.carseats=tree(High~.-Sales,Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats,pretty=0)
tree.carseats
set.seed(2)
train=sample(1:nrow(Carseats), 200)
Carseats.test=Carseats[-train,]
High.test=High[-train]
tree.carseats=tree(High~.-Sales,Carseats,subset=train)
tree.pred=predict(tree.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
(86+57)/200
set.seed(3)
cv.carseats=cv.tree(tree.carseats,FUN=prune.misclass)
names(cv.carseats)
cv.carseats
par(mfrow=c(1,2))
plot(cv.carseats$size,cv.carseats$dev,type="b")
plot(cv.carseats$k,cv.carseats$dev,type="b")
prune.carseats=prune.misclass(tree.carseats,best=9)
plot(prune.carseats)
text(prune.carseats,pretty=0)
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
(94+60)/200
prune.carseats=prune.misclass(tree.carseats,best=15)
plot(prune.carseats)
text(prune.carseats,pretty=0)
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
(86+62)/200
# Fitting Regression Trees
library(MASS)
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston=tree(medv~.,Boston,subset=train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston,pretty=0)
cv.boston=cv.tree(tree.boston)
plot(cv.boston$size,cv.boston$dev,type='b')
prune.boston=prune.tree(tree.boston,best=5)
plot(prune.boston)
text(prune.boston,pretty=0)
yhat=predict(tree.boston,newdata=Boston[-train,])
boston.test=Boston[-train,"medv"]
plot(yhat,boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
# Bagging and Random Forests
library(randomForest)
set.seed(1)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,importance=TRUE)
bag.boston
yhat.bag = predict(bag.boston,newdata=Boston[-train,])
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag-boston.test)^2)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,ntree=25)
yhat.bag = predict(bag.boston,newdata=Boston[-train,])
mean((yhat.bag-boston.test)^2)
set.seed(1)
rf.boston=randomForest(medv~.,data=Boston,subset=train,mtry=6,importance=TRUE)
yhat.rf = predict(rf.boston,newdata=Boston[-train,])
mean((yhat.rf-boston.test)^2)
importance(rf.boston)
varImpPlot(rf.boston)
# Boosting
library(gbm)
set.seed(1)
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000,interaction.depth=4)
summary(boost.boston)
par(mfrow=c(1,2))
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
yhat.boost=predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((yhat.boost-boston.test)^2)
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000,interaction.depth=4,shrinkage=0.2,verbose=F)
yhat.boost=predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((yhat.boost-boston.test)^2)
setwd("C:/Users/abhij/Desktop/UVa Coursework/SYS 6018/Competitions/sys6018-competition-safe-driver-predictions")
library(randomForest)
library(foreach)
#-------------------------------------------DATA CLEANING------------------------------------------------
train1 <- read.csv('train.csv', header=T) #reading in the data
test1 <- read.csv('test.csv', header=T)
all_data <- rbind(train1[,-2], test1)
#function to get the mode
getmode <- function(v) {
uniqv <- unique(v)
v <- v[v!=-1]
uniqv[which.max(tabulate(match(v, uniqv)))]
}
#REMOVING -1 VALUES
#Here I am removing -1 values for categorical variables and imputing the mode
#I will also be removing -1 values for numerical variables and imputing the mean
sum(all_data == -1) #2116753
cat1 <- c(3,5:14, 17:19, 23:33, 53:58) #create list of cat variables
all_data[cat1] <- lapply(all_data[cat1], as.factor)
num1 <- c(2,4, 15:16, 20:22, 34:52) #create list of num variables
all_data[num1] <- lapply(all_data[num1], as.numeric)
#Impute Mode for factor/Mean for numeric where row value = -1 or missing
for (i in 2:58){
if (class(all_data[,i]) == 'factor'){
all_data[,i][all_data[,i] == -1] <- getmode(all_data[,i])
}
else if (class(all_data[,i]) == 'numeric'){
all_data[,i][all_data[,i] == -1] <- mean(all_data[,i])
}
}
#Check if -1
sum(all_data == -1) #0
#Drop unused levels
all_data <- droplevels.data.frame(all_data)
#check level count
col_levels <- lapply(all_data[,c(3,5:14, 17:19, 23:33, 53:58)], function(x) nlevels(x))
#Check which column has a level count higher than 53(max levels Random forest works with)
which(col_levels>53) #ps_car_11_cat
#Drop column from all data
all_data <- within(all_data, rm(ps_car_11_cat))
#Separate data back to Train and Test
train <- all_data[1:595212,]
test <- all_data[595213:1488028,]
train <- as.data.frame(cbind(train1$target, train))
colnames(train)[1] <- "target"
#Remove unused data frames
rm(train1, test1, all_data)
#Subset of 10,000 observations
train_subset <- sample(1:nrow(train), 1000, replace= FALSE)
train_subset <- train[train_subset,]
#Create data sets for random forest
sample = sample(dim(train_subset)[1], dim(train_subset)[1]/2)
X.train = train_subset[sample, -59]
X.test = train_subset[-sample, -59]
Y.train = train_subset[sample, 59]
Y.test = train_subset[-sample, 59]
#Create different values for m to test
p = dim(train_subset)[2] - 1
p2 = round(p/2)
psq = round(sqrt(p))
